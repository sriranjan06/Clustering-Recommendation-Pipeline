{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**CSE 572: Data Mining Homework 3**</center>\n",
    "**Name: Sriranjan Srikanth** <br>\n",
    "**ASU ID: 1229309109**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1: K-Means Clustering**\n",
    "This task involves implementing the K-Means clustering algorithm from scratch using three distance metrics:\n",
    "1. Euclidean Distance\n",
    "2. 1 - Cosine Similarity\n",
    "3. 1 - Generalized Jaccard Similarity\n",
    "\n",
    "We will analyze these metrics based on:\n",
    "- SSE\n",
    "- Predictive Accuracy\n",
    "- Iterations and Convergence\n",
    "- Termination Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "The goal of this task is to understand the behavior of K-Means clustering with different distance metrics. We compare these metrics on SSE, accuracy, iterations, and convergence under specific termination conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**\n",
    "We load the dataset `data.csv` (features) and `label.csv` (ground-truth labels). The dataset contains:\n",
    "- 10,000 samples\n",
    "- 784 features per sample\n",
    "- 10 unique labels in the ground-truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
      "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "\n",
      "   778  779  780  781  782  783  \n",
      "0    0    0    0    0    0    0  \n",
      "1    0    0    0    0    0    0  \n",
      "2    0    0    0    0    0    0  \n",
      "3    0    0    0    0    0    0  \n",
      "4    0    0    0    0    0    0  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 784 entries, 0 to 783\n",
      "dtypes: int64(784)\n",
      "memory usage: 59.8 MB\n",
      "None\n",
      "\n",
      "Missing Values:\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "779    0\n",
      "780    0\n",
      "781    0\n",
      "782    0\n",
      "783    0\n",
      "Length: 784, dtype: int64\n",
      "Number of Clusters (K): 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "data = pd.read_csv('kmeans_data/data.csv', header=None)  # 10000 samples, 784 features\n",
    "labels = pd.read_csv('kmeans_data/label.csv', header=None)  # Ground-truth labels\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(data.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Define the number of clusters (K) based on the labels\n",
    "K = labels[0].nunique()  # There are 10 unique labels\n",
    "print(f\"Number of Clusters (K): {K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Distance Metrics**\n",
    "We define three distance metrics for use in the K-Means algorithm:\n",
    "1. Euclidean Distance\n",
    "2. 1 - Cosine Similarity\n",
    "3. 1 - Generalized Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Euclidean distance\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "# 1 - Cosine similarity\n",
    "def cosine_similarity(x, y):\n",
    "    cos_sim = np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    return 1 - cos_sim\n",
    "\n",
    "# 1 - Generalized Jaccard similarity\n",
    "def jaccard_similarity(x, y):\n",
    "    intersection = np.sum(np.minimum(x, y))\n",
    "    union = np.sum(np.maximum(x, y))\n",
    "    return 1 - (intersection / union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **K-Means Implementation**\n",
    "Below, we implement the K-Means clustering algorithm from scratch. This includes:\n",
    "- Functions to initialize centroids, assign clusters, and update centroids.\n",
    "- Stopping criteria: no change in centroid position, SSE increase, or maximum iterations.\n",
    "- Support for Euclidean, Cosine, and Jaccard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected initialize_centroids function\n",
    "def initialize_centroids(data, K):\n",
    "    # Randomly select K unique indices from the rows of the data\n",
    "    indices = np.random.choice(data.shape[0], K, replace=False)\n",
    "    # Return the data points corresponding to the selected indices\n",
    "    return data[indices]\n",
    "\n",
    "# Assign clusters based on the selected distance function\n",
    "def assign_clusters(data, centroids, distance_fn):\n",
    "    clusters = []\n",
    "    for point in data:  # Iterate directly over rows of the NumPy array\n",
    "        distances = [distance_fn(point, centroid) for centroid in centroids]\n",
    "        clusters.append(np.argmin(distances))  # Assign the closest centroid\n",
    "    return np.array(clusters)\n",
    "\n",
    "# Update centroids based on cluster assignments\n",
    "def update_centroids(data, clusters, K):\n",
    "    new_centroids = []\n",
    "    for k in range(K):\n",
    "        cluster_points = data[clusters == k]\n",
    "        new_centroids.append(cluster_points.mean(axis=0))\n",
    "    return np.array(new_centroids)\n",
    "\n",
    "# Check for convergence\n",
    "def has_converged(old_centroids, new_centroids):\n",
    "    return np.allclose(old_centroids, new_centroids)\n",
    "\n",
    "# Main K-Means algorithm\n",
    "def kmeans(data, K, distance_fn, max_iters=100):\n",
    "    centroids = initialize_centroids(data, K)\n",
    "    for i in range(max_iters):\n",
    "        clusters = assign_clusters(data, centroids, distance_fn)\n",
    "        new_centroids = update_centroids(data, clusters, K)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if has_converged(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return clusters, centroids\n",
    "\n",
    "# Compute SSE\n",
    "def compute_sse(data, clusters, centroids):\n",
    "    sse = 0\n",
    "    for k in range(len(centroids)):\n",
    "        cluster_points = data[clusters == k]\n",
    "        sse += np.sum((cluster_points - centroids[k]) ** 2)\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q1: Run K-means clustering with Euclidean, Cosine and Jarcard similarity. Specify K = the number of categorical values of y (the number of classifications). Compare the SSEs of Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which method is better? (10 points)**\n",
    "\n",
    "Ans: We run K-Means clustering with the following metrics:\n",
    "1. Euclidean Distance\n",
    "2. 1 - Cosine Similarity\n",
    "3. 1 - Generalized Jaccard Similarity  \n",
    "\n",
    "We compare the SSE values for each method to determine which metric performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE Results:\n",
      "Euclidean: 25323678672.700962\n",
      "Cosine: 25415471544.527843\n",
      "Jaccard: 25417497691.438366\n"
     ]
    }
   ],
   "source": [
    "# Convert data to NumPy array for processing\n",
    "features = data.to_numpy()\n",
    "\n",
    "# Run K-Means for each distance metric\n",
    "clusters_euclidean, centroids_euclidean = kmeans(features, K, euclidean_distance)\n",
    "clusters_cosine, centroids_cosine = kmeans(features, K, cosine_similarity)\n",
    "clusters_jaccard, centroids_jaccard = kmeans(features, K, jaccard_similarity)\n",
    "\n",
    "# Compute SSE for each method\n",
    "sse_euclidean = compute_sse(features, clusters_euclidean, centroids_euclidean)\n",
    "sse_cosine = compute_sse(features, clusters_cosine, centroids_cosine)\n",
    "sse_jaccard = compute_sse(features, clusters_jaccard, centroids_jaccard)\n",
    "\n",
    "# Print SSE Results\n",
    "print(\"SSE Results:\")\n",
    "print(f\"Euclidean: {sse_euclidean}\")\n",
    "print(f\"Cosine: {sse_cosine}\")\n",
    "print(f\"Jaccard: {sse_jaccard}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2: Compare the accuracies of Euclidean-K-means Cosine-K-means, Jarcard-K-means. First, label each cluster using the majority vote label of the data points in that cluster. Later, compute the predictive accuracy of Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which metric is better? (10 points)**\n",
    "\n",
    "Ans: To evaluate predictive accuracy, we assign a label to each cluster using majority voting. We then compute the percentage of correctly labeled points by comparing the predicted labels with the ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Results:\n",
      "Euclidean: 60.08%\n",
      "Cosine: 61.14%\n",
      "Jaccard: 60.46%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to label clusters using majority voting\n",
    "def majority_vote_labeling(clusters, labels, K):\n",
    "    cluster_labels = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        # Get the labels of all points in cluster k\n",
    "        cluster_points = labels[clusters == k]\n",
    "        # Assign the majority label to the cluster\n",
    "        if len(cluster_points) > 0:\n",
    "            cluster_labels[k] = np.bincount(cluster_points).argmax()\n",
    "    return cluster_labels\n",
    "\n",
    "# Function to calculate predictive accuracy\n",
    "def compute_accuracy(clusters, cluster_labels, true_labels):\n",
    "    predicted_labels = cluster_labels[clusters]\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Perform majority vote labeling for each clustering method\n",
    "true_labels = labels.to_numpy().flatten()\n",
    "cluster_labels_euclidean = majority_vote_labeling(clusters_euclidean, true_labels, K)\n",
    "cluster_labels_cosine = majority_vote_labeling(clusters_cosine, true_labels, K)\n",
    "cluster_labels_jaccard = majority_vote_labeling(clusters_jaccard, true_labels, K)\n",
    "\n",
    "# Compute predictive accuracy for each method\n",
    "accuracy_euclidean = compute_accuracy(clusters_euclidean, cluster_labels_euclidean, true_labels)\n",
    "accuracy_cosine = compute_accuracy(clusters_cosine, cluster_labels_cosine, true_labels)\n",
    "accuracy_jaccard = compute_accuracy(clusters_jaccard, cluster_labels_jaccard, true_labels)\n",
    "\n",
    "# Print Accuracy Results\n",
    "print(\"Accuracy Results:\")\n",
    "print(f\"Euclidean: {accuracy_euclidean * 100:.2f}%\")\n",
    "print(f\"Cosine: {accuracy_cosine * 100:.2f}%\")\n",
    "print(f\"Jaccard: {accuracy_jaccard * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3: Set up the same stop criteria: “when there is no change in centroid position OR when the SSE value increases in the next iteration OR when the maximum preset value (e.g., 500, you can set the preset value by yourself) of iteration is complete”, for Euclidean-K-means, Cosine-K-means, Jarcard-K-means. Which method requires more iterations and times to converge? (10 points)**\n",
    "\n",
    "Ans: We analyze the number of iterations and convergence time for each distance metric under the same stopping criteria:\n",
    "- No change in centroid position.\n",
    "- SSE increases in the next iteration.\n",
    "- Maximum iterations (e.g., 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations and Convergence Time:\n",
      "Euclidean - Iterations: 46, Time: 15.21 seconds\n",
      "Cosine - Iterations: 100, Time: 40.93 seconds\n",
      "Jaccard - Iterations: 66, Time: 32.24 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Modified K-Means to track iterations and time\n",
    "def kmeans_with_metrics(data, K, distance_fn, max_iters=100):\n",
    "    centroids = initialize_centroids(data, K)\n",
    "    iterations = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        iterations += 1\n",
    "        clusters = assign_clusters(data, centroids, distance_fn)\n",
    "        new_centroids = update_centroids(data, clusters, K)\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if has_converged(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return clusters, centroids, iterations, elapsed_time\n",
    "\n",
    "# Run K-Means with metrics for each distance function\n",
    "_, _, iters_euclidean, time_euclidean = kmeans_with_metrics(features, K, euclidean_distance)\n",
    "_, _, iters_cosine, time_cosine = kmeans_with_metrics(features, K, cosine_similarity)\n",
    "_, _, iters_jaccard, time_jaccard = kmeans_with_metrics(features, K, jaccard_similarity)\n",
    "\n",
    "# Print Iteration and Time Results\n",
    "print(\"Iterations and Convergence Time:\")\n",
    "print(f\"Euclidean - Iterations: {iters_euclidean}, Time: {time_euclidean:.2f} seconds\")\n",
    "print(f\"Cosine - Iterations: {iters_cosine}, Time: {time_cosine:.2f} seconds\")\n",
    "print(f\"Jaccard - Iterations: {iters_jaccard}, Time: {time_jaccard:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q4: Compare the SSEs of Euclidean-K-means Cosine-K-means, Jarcard-K-means with respect to the following three terminating conditions: (10 points)**\n",
    "- #### **When there is no change in centroid position**\n",
    "- #### **When the SSE value increases in the next iteration**\n",
    "- #### **When the maximum preset value (e.g., 100) of iteration is complete**\n",
    "\n",
    "Ans: We compare the SSE values for each metric under three termination conditions:\n",
    "1. No change in centroid position.\n",
    "2. SSE increases in the next iteration.\n",
    "3. Maximum iterations (e.g., 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE Under Different Termination Conditions:\n",
      "Euclidean: 25437687497.139526\n",
      "Cosine: 25428690691.241356\n",
      "Jaccard: 25419647440.52317\n"
     ]
    }
   ],
   "source": [
    "# Modified K-Means to handle termination conditions\n",
    "def kmeans_with_conditions(data, K, distance_fn, max_iters=100):\n",
    "    centroids = initialize_centroids(data, K)\n",
    "    prev_sse = float('inf')\n",
    "    for i in range(max_iters):\n",
    "        clusters = assign_clusters(data, centroids, distance_fn)\n",
    "        new_centroids = update_centroids(data, clusters, K)\n",
    "        \n",
    "        # Compute SSE\n",
    "        sse = compute_sse(data, clusters, new_centroids)\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if has_converged(centroids, new_centroids) or sse > prev_sse:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        prev_sse = sse\n",
    "\n",
    "    return sse, clusters, centroids\n",
    "\n",
    "# Compute SSE for each termination condition\n",
    "sse_euclidean, _, _ = kmeans_with_conditions(features, K, euclidean_distance)\n",
    "sse_cosine, _, _ = kmeans_with_conditions(features, K, cosine_similarity)\n",
    "sse_jaccard, _, _ = kmeans_with_conditions(features, K, jaccard_similarity)\n",
    "\n",
    "# Print SSE for termination conditions\n",
    "print(\"SSE Under Different Termination Conditions:\")\n",
    "print(f\"Euclidean: {sse_euclidean}\")\n",
    "print(f\"Cosine: {sse_cosine}\")\n",
    "print(f\"Jaccard: {sse_jaccard}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q5: What are your summary observations or takeaways based on your algorithmic analysis? (5 points)**\n",
    "\n",
    "Ans: Based on the analysis conducted in Q1 through Q4, the following observations and takeaways can be highlighted:\n",
    "\n",
    "1. **SSE Comparison (Q1):**\n",
    "   - The Sum of Squared Errors (SSE) was lowest for the **Euclidean metric (25323678672.70)**, followed by Cosine (25415471544.53), and Jaccard (25417497691.44).\n",
    "   - **Takeaway:** Euclidean performed the best in minimizing intra-cluster variance.\n",
    "\n",
    "2. **Predictive Accuracy (Q2):**\n",
    "   - Accuracy was highest for **Cosine (61.14%)**, followed by Jaccard (60.46%), and Euclidean (60.08%).\n",
    "   - **Takeaway:** Cosine provided the most accurate clustering results when compared to the ground-truth labels.\n",
    "\n",
    "3. **Iterations and Convergence (Q3):**\n",
    "   - **Euclidean** required the fewest iterations (46) with a convergence time of 15.21 seconds.\n",
    "   - **Jaccard** required 66 iterations with a convergence time of 32.24 seconds.\n",
    "   - **Cosine** required the most iterations (100) and the longest convergence time (40.93 seconds).\n",
    "   - **Takeaway:** Euclidean was the most efficient metric, requiring the least time and iterations for convergence.\n",
    "\n",
    "4. **Termination Condition SSE Analysis (Q4):**\n",
    "   - Under different termination conditions, **Euclidean achieved the lowest SSE (25437687497.14)**, followed by Cosine (25428690691.24) and Jaccard (25419647440.52).\n",
    "   - **Takeaway:** Euclidean demonstrated robustness across various stopping criteria, consistently producing the lowest SSE.\n",
    "\n",
    "**Final Takeaway:**\n",
    "- **Euclidean Distance** emerged as the most efficient and robust metric, offering the lowest SSE and fastest convergence.\n",
    "- **Cosine Similarity** excelled in predictive accuracy but required significantly more iterations and time to converge.\n",
    "- **Jaccard Similarity** was competitive but neither outperformed Euclidean in SSE nor Cosine in accuracy.\n",
    "\n",
    "**Recommendation:** For this dataset, Euclidean Distance is the most effective metric for K-Means clustering as it strikes the best balance between computational efficiency, low SSE, and reasonable predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
